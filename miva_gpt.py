# -*- coding: utf-8 -*-
"""MIVA_GPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GqjhNxEy2XwST8fukqc-qMbCl96gCzN0
"""

from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch
from PIL import Image

cap_model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
cap_tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cap_model.to(device)

max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}
def predict_step(img_frame):
  pixel_values = feature_extractor(images=img_frame, return_tensors="pt").pixel_values
  pixel_values = pixel_values.to(device)

  output_ids = cap_model.generate(pixel_values, **gen_kwargs)

  preds = cap_tokenizer.batch_decode(output_ids, skip_special_tokens=True)
  preds = [pred.strip() for pred in preds]
  return preds

!pip install av

import av
import numpy as np

def generate_captions(video_path):
  container = av.open(video_path)
  # Extract evenly spaced frames from video
  seg_len = container.streams.video[0].frames
  clip_len = 30
  indices = set(np.linspace(0, seg_len, num=clip_len, endpoint=False).astype(np.int64))
  frames = []
  container.seek(0)

  for i, frame in enumerate(container.decode(video=0)):
      if i in indices:
          frames.append(frame.to_ndarray(format="rgb24"))
  # print(frames)
  # Generate caption
  final_sentences = []
  gen_kwargs = {
      "min_length": 10,
      "max_length": 20,
      "num_beams": 8,
  }
  for frame in frames:
        # Preprocess the frame
        pil_image = Image.fromarray(frame)
        pixel_values = feature_extractor(images=pil_image, return_tensors="pt").pixel_values.to(device)

        # Generate caption for the frame
        tokens = cap_model.generate(pixel_values, **gen_kwargs)
        caption = cap_tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]

        # Append the caption to the list if it's not already present
        if caption not in final_sentences:
            final_sentences.append(caption)
  return final_sentences

import torch
import librosa
import moviepy.editor as mp
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the Wav2Vec2 processor and model
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
att_model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h").to(device)

# Function to resample audio to match the model's sampling rate
def resample_audio(audio, original_sr, target_sr):
    return librosa.resample(audio, orig_sr=original_sr, target_sr=target_sr)

# Function to transcribe audio from video
def transcribe_video_audio(video_path):
    # Extract audio from video
    video = mp.VideoFileClip(video_path)
    audio_path = "temp_audio.wav"
    video.audio.write_audiofile(audio_path, codec='pcm_s16le', ffmpeg_params=['-ac', '1'])

    # Load audio using librosa and resample
    audio, sr = librosa.load(audio_path, sr=None)
    audio = resample_audio(audio, sr, 16000)  # Resample audio to 16000 Hz

    # Tokenize and convert audio to input features
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)

    # Transcribe audio to text
    with torch.no_grad():
        logits = att_model(input_values=inputs.input_values.to(device)).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    return transcription

video_path = input('Enter path of the video : ')

gc = generate_captions(video_path)
unique_captions = list(dict.fromkeys(gc))
sentences_with_period = [sentence + '.' for sentence in unique_captions]
captioned_summary = ' '.join(sentences_with_period)
captioned_summary

transcribed_text = transcribe_video_audio(video_path)
print(transcribed_text)

!pip install openai

from openai import OpenAI

client = OpenAI(api_key='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')
# The text paragraph you want the chatbot to reference
#context = captioned_summary + " " + transcribed_text[0]

def ask_question(question,context,choices):
    choices_str = ", ".join(choices)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
            {"role": "system", "content": "You are a helpful assistant. You are given a context and asked a question. Answer question with only one word from the given choices."},
            {"role": "user", "content": choices_str},
            {"role": "user", "content": context},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# Example of asking a question
# question = input("Ask your Question : ")
# answer = ask_question(question,context)
# print("Answer:", answer)

test_videos_path = '/content/drive/MyDrive/MSRVTT-QA/TestVideo'
test_videos_results_path = '/content/drive/MyDrive/MSRVTT-QA/test_qa.json'

import json
with open(test_videos_results_path) as f:
  data = json.load(f)

choices = [entry['answer'] for entry in data]
choices = list(set(choices))

def fetch_questions_by_video_id(video_id, data):
    questions = [entry['question'] for entry in data if entry['video_id'] == video_id]
    return questions

def evaluate_response(model_response, ground_truth_answer):
    # You might want to add more sophisticated evaluation logic here
    return model_response.strip().lower() == ground_truth_answer.strip().lower()

import os

model_responses = []
ground_truth_responses = []
for video in os.listdir(test_videos_path):
    video_path = os.path.join(test_videos_path, video)
    video_id = int(video[5:9])
    gc = generate_captions(video_path)
    unique_captions = list(dict.fromkeys(gc))
    sentences_with_period = [sentence + '.' for sentence in unique_captions]
    captioned_summary = ' '.join(sentences_with_period)
    transcribed_text = transcribe_video_audio(video_path)
    context = captioned_summary + " " + transcribed_text[0]
    questions = fetch_questions_by_video_id(video_id,data)
    for question in questions:
      model_response = ask_question(question,context,choices)
      ground_truth_answer = [entry['answer'] for entry in data if entry['video_id'] == video_id and entry['question'] == question][0]
      # Evaluate the model response
      # is_correct = evaluate_response(model_response, ground_truth_answer)
      # Store model response and ground truth response
      model_responses.append(model_response)
      ground_truth_responses.append(ground_truth_answer)











"""LLAMA Implementation"""

# !pip install accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3
# !pip install bitsandbytes==0.40.0 einops==0.6.1
# !pip install xformers==0.0.22.post7
# !pip install langchain==0.1.4
# !pip install faiss-gpu==1.7.1.post3
# !pip install sentence_transformers

# from torch import cuda, bfloat16
# import transformers

# model_id = 'meta-llama/Llama-2-7b-chat-hf'
# #model_id = 'GPT2'
# device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
# #device = cpu
# # set quantization configuration to load large model with less GPU memory
# # this requires the `bitsandbytes` library
# bnb_config = transformers.BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type='nf4',
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_compute_dtype=bfloat16
# )

# # begin initializing HF items, you need an access token
# hf_auth = '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'
# model_config = transformers.AutoConfig.from_pretrained(
#     model_id,
#     use_auth_token=hf_auth
# )

# model = transformers.AutoModelForCausalLM.from_pretrained(
#     model_id,
#     trust_remote_code=True,
#     config=model_config,
#     quantization_config=bnb_config,
#     device_map='auto',
#     use_auth_token=hf_auth
# )

# # enable evaluation mode to allow model inference
# model.eval()

# print(f"Model loaded on {device}")

# tokenizer = transformers.AutoTokenizer.from_pretrained(
#     model_id,
#     use_auth_token=hf_auth
# )

# generator = transformers.pipeline(
#     model = model,
#     tokenizer=tokenizer,
#     return_full_text = True, # langchain expects full text
#     task='text-generation',
#     #stopping_criteria=stopping_criteria, # without this model rambles during chat
#     temperature=0.1, # 'randomness' of outputs, 0.0 is the min and 1.0 is the max
#     max_new_tokens=512, # max number of tokens to generate in the output
#     repetition_penalty=1.1 # without this output begins repeating
# )

# res = generator("Explain me the difference between Data Lakehouse and Data Warehouse.")
# print(res[0]["generated_text"])

# # prompt: Using captioned_summary and transcribed_text build me a question answer system

# from langchain.chains import ConversationalRetrievalChain, RetrievalQA
# from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, CONDENSE_QUESTION_AND_ANSWER_PROMPT
# from langchain.chains.conversational_retrieval.retrievers import GPT3SimpleVectorIndexRetriever
# from langchain.memory import ConversationBufferMemory
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.vectorstores import FAISS
# from langchain.chat_models import ChatOpenAI
# from langchain.llms import OpenAI
# from langchain import OpenAI, VectorDBQA, HuggingFacePipeline
# from langchain.document_loaders import TextLoader

# # Load the data
# loader = TextLoader('../../data/meteoric.txt')
# documents = loader.load()

# # Create the text splitter
# text_splitter = CharacterTextSplitter()
# texts = text_splitter.split_documents(documents)

# # Create the embedding function
# embeddings = OpenAIEmbeddings()

# # Create the vector store
# vectorstore = FAISS.from_embeddings(embeddings, texts)

# # Create the retriever
# retriever = GPT3SimpleVectorIndexRetriever.from_vectorstore(vectorstore)

# # Create the prompts
# condenser_prompt = CONDENSE_QUESTION_AND_ANSWER_PROMPT
# question_prompt = CONDENSE_QUESTION_PROMPT

# # Create the memory
# memory = ConversationBufferMemory(memory_key="chat_history", input_key="user_input", output_key="assistant_output")

# # Create the conversational retrieval chain
# chain = ConversationalRetrievalChain(
#     retriever=retriever,
#     prompt=question_prompt,
#     memory=memory,
#     chat_model=ChatOpenAI(
#         engine=OpenAI(lang="en"),
#         prompt=condenser_prompt,
#         temperature=0.5,
#     ),
# )

# # Start the chain
# chain.start(captioned_summary + transcribed_text)

# # Ask a question
# question = input("Ask a question: ")

# # Get the answer
# answer = chain.run(question)

# # Print the answer
# print(answer)

# import locale
# locale.getpreferredencoding = lambda: "UTF-8"

# from langchain.chains import RetrievalQA
# from langchain.chains.conversational_retrieval.retrievers import GPT3SimpleVectorIndexRetriever
# from langchain.memory import ConversationBufferMemory
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.vectorstores import FAISS
# from langchain.chat_models import ChatOpenAI
# from langchain.llms import OpenAI

# # Replace these with your actual summaries
# # captioned_summary = "Your captioned summary here"
# # transcribed_text = "Your transcribed text here"

# # Combine the summaries
# combined_text = captioned_summary + " " + transcribed_text[0]

# # Create the text splitter
# text_splitter = CharacterTextSplitter()
# texts = text_splitter.split_text(combined_text)

# # Create the embedding function
# embeddings = OpenAIEmbeddings()

# # Create the vector store
# vectorstore = FAISS.from_embeddings(embeddings, texts)

# # Create the retriever
# retriever = GPT3SimpleVectorIndexRetriever.from_vectorstore(vectorstore)

# # Create the memory
# memory = ConversationBufferMemory(memory_key="chat_history", input_key="user_input", output_key="assistant_output")

# # Create the QA chain
# chain = RetrievalQA(
#     retriever=retriever,
#     memory=memory,
#     chat_model=ChatOpenAI(
#         engine=OpenAI(lang="en"),
#         prompt_template="Q: {question}\nA:",
#         temperature=0.5,
#     ),
# )

# # Start the chain
# chain.start(combined_text)

# # Ask a question
# question = input("Ask a question: ")

# # Get the answer
# answer = chain.run(question)

# # Print the answer
# print(answer)

# from langchain.llms import HuggingFacePipeline
# from langchain.chains import ConversationalRetrievalChain
# from langchain.prompts import PromptTemplate
# #
# # # wrap the transformer model with Huggingface pipeline so that we can use prompt later
# # llm = HuggingFacePipeline(pipeline=generator)
# # # creating prompt for large language model
# # pre_prompt = """[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\nGenerate the next agent response by answering the question. Answer it as succinctly as possible. You are provided several documents with titles. If the answer comes from different documents please mention all possibilities in your answer and use the titles to separate between topics or domains. If you cannot answer the question from the given documents, please state that you do not have an answer.\n"""
# # prompt = pre_prompt + "CONTEXT:\n\n{context}\n" +"Question : {question}" + "[\INST]"
# # llama_prompt = PromptTemplate(template=prompt, input_variables=["context", "question"])
# # # integrate prompt with LLM
# # chain = ConversationalRetrievalChain.from_llm(llm, loaded_vectorstore.as_retriever(), combine_docs_chain_kwargs={"prompt": llama_prompt}, return_source_documents=True)

# llm = HuggingFacePipeline(pipeline=generator)
# # checking again that everything is working fine
# llm(prompt="Explain me the difference between Data Lakehouse and Data Warehouse.")

# del cap_model
# del feature_extractor
# del cap_tokenizer
# del att_model
# del processor
# torch.cuda.empty_cache()

# import os

# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:200"

# pip install gpt-index

# pip install llama-index==0.8.42

# from langchain.chains import RetrievalQA
# from llama_index import GPTVectorStoreIndex
# from langchain.memory import ConversationBufferMemory
# from langchain.chat_models import ChatOpenAI
# from langchain.llms import OpenAI

# import os
# os.environ["OPENAI_API_KEY"] = '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'
# # Replace these with your actual summaries
# # captioned_summary = "Your captioned summary here"
# # transcribed_text = "Your transcribed text here"

# # Combine the summaries
# combined_text = captioned_summary + " " + transcribed_text[0]

# # Create the retriever
# retriever = GPTVectorStoreIndex()

# # Create the memory
# memory = ConversationBufferMemory(memory_key="chat_history", input_key="user_input", output_key="assistant_output")

# # Create the QA chain
# chain = RetrievalQA(
#     retriever=retriever,
#     memory=memory,
#     chat_model=ChatOpenAI(
#         engine=OpenAI(lang="en"),
#         prompt_template="Q: {question}\nA:",
#         temperature=0.5,
#     ),
# )

# # Start the chain
# chain.start(combined_text)

# # Ask a question
# question = input("Ask a question: ")

# # Get the answer
# answer = chain.run(question)

# # Print the answer
# print(answer)



